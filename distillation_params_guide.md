# 知识蒸馏关键参数设置指南

## 参数位置和作用

### 1. 蒸馏温度 (T) - `distill_temperature`

**位置**: 
- 命令行参数: `--distill_temperature`
- 默认值: `2.0`
- 代码位置: `model.py` 第17行和第151行

**作用**:
- 控制知识蒸馏中softmax的"软化"程度
- 较高的温度使输出分布更平滑，传递更多的"暗知识"
- 较低的温度使输出更接近硬标签

**推荐值**:
- `T = 1.0`: 等价于硬标签，无蒸馏效果
- `T = 2.0-4.0`: 常用范围，平衡性能和知识传递
- `T = 5.0-10.0`: 高温度，传递更多细粒度知识
- `T > 10.0`: 可能导致训练不稳定

### 2. 蒸馏损失权重 (λ) - `distill_lambda`

**位置**:
- 命令行参数: `--distill_lambda`
- 默认值: `1.0`
- 代码位置: `model.py` 第18行和第154行

**作用**:
- 平衡蒸馏损失和分类损失的重要性
- 总损失 = λ × 蒸馏损失 + 分类损失

**推荐值**:
- `λ = 0.0`: 无蒸馏，只使用分类损失
- `λ = 0.1-0.5`: 轻度蒸馏，主要关注新任务
- `λ = 1.0`: 平衡蒸馏和分类
- `λ = 2.0-5.0`: 强蒸馏，重点保持旧知识
- `λ > 5.0`: 可能导致新任务学习困难

## 使用示例

### 命令行使用:
```bash
# 标准设置
python main.py --distill_temperature 2.0 --distill_lambda 1.0

# 强蒸馏设置（更好地保持旧知识）
python main.py --distill_temperature 4.0 --distill_lambda 2.0

# 轻蒸馏设置（更关注新任务）
python main.py --distill_temperature 1.5 --distill_lambda 0.3

# 无蒸馏（对比实验）
python main.py --distill_lambda 0.0
```

### 代码中直接设置:
```python
class Args:
    def __init__(self):
        self.distill_temperature = 3.0  # 蒸馏温度
        self.distill_lambda = 0.8       # 蒸馏权重
        # ... 其他参数
```

## 参数调优建议

### 1. 温度调优策略:
- 从 T=2.0 开始
- 如果遗忘严重，增加到 T=3.0-4.0
- 如果新任务学习困难，降低到 T=1.5-2.0

### 2. λ调优策略:
- 从 λ=1.0 开始
- 观察旧任务准确率下降情况
- 如果遗忘严重，增加 λ 到 1.5-2.0
- 如果新任务学习缓慢，降低 λ 到 0.5-0.8

### 3. 联合调优:
- 高温度 + 高λ: 最大化知识保持
- 低温度 + 低λ: 最大化新任务学习
- 中等设置: 平衡性能

## 实验监控

训练时会输出损失信息:
```
Distill Loss: 0.8234, Cls Loss: 1.2456, λ: 1.0, T: 2.0
```

关注指标:
- 蒸馏损失应该逐渐下降
- 分类损失应该快速下降
- 两者比例应该合理（不要相差过大）

## 不同场景的推荐设置

| 场景 | T | λ | 说明 |
|------|---|---|------|
| 标准持续学习 | 2.0 | 1.0 | 平衡设置 |
| 严重遗忘问题 | 4.0 | 2.0 | 强化知识保持 |
| 新任务困难 | 1.5 | 0.5 | 优先新任务学习 |
| 快速原型 | 1.0 | 0.1 | 轻量蒸馏 |
| 对比实验 | - | 0.0 | 无蒸馏基线 |